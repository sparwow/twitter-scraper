{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter scraper.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sparwow/twitter-scraper/blob/main/twitter_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B9PmSdBcxGGM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b283698c-bd21-48dd-ab9b-2e121a49277b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [1 \r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Co\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rIgn:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rGet:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:8 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [867 kB]\n",
            "Hit:13 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,459 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [725 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,498 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,822 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,935 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [758 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,238 kB]\n",
            "Get:24 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [934 kB]\n",
            "Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [45.3 kB]\n",
            "Fetched 14.6 MB in 4s (3,414 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3.8-minimal\n",
            "Suggested packages:\n",
            "  python3.8-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.8-minimal libpython3.8-stdlib python3.8 python3.8-minimal\n",
            "0 upgraded, 4 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 4,676 kB of archives.\n",
            "After this operation, 18.5 MB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 libpython3.8-minimal amd64 3.8.12-1+bionic3 [762 kB]\n",
            "Get:2 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.8-minimal amd64 3.8.12-1+bionic3 [1,825 kB]\n",
            "Get:3 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 libpython3.8-stdlib amd64 3.8.12-1+bionic3 [1,656 kB]\n",
            "Get:4 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 python3.8 amd64 3.8.12-1+bionic3 [433 kB]\n",
            "Fetched 4,676 kB in 5s (1,007 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 4.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.8-minimal:amd64.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../libpython3.8-minimal_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking libpython3.8-minimal:amd64 (3.8.12-1+bionic3) ...\n",
            "Selecting previously unselected package python3.8-minimal.\n",
            "Preparing to unpack .../python3.8-minimal_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking python3.8-minimal (3.8.12-1+bionic3) ...\n",
            "Selecting previously unselected package libpython3.8-stdlib:amd64.\n",
            "Preparing to unpack .../libpython3.8-stdlib_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking libpython3.8-stdlib:amd64 (3.8.12-1+bionic3) ...\n",
            "Selecting previously unselected package python3.8.\n",
            "Preparing to unpack .../python3.8_3.8.12-1+bionic3_amd64.deb ...\n",
            "Unpacking python3.8 (3.8.12-1+bionic3) ...\n",
            "Setting up libpython3.8-minimal:amd64 (3.8.12-1+bionic3) ...\n",
            "Setting up python3.8-minimal (3.8.12-1+bionic3) ...\n",
            "Setting up libpython3.8-stdlib:amd64 (3.8.12-1+bionic3) ...\n",
            "Setting up python3.8 (3.8.12-1+bionic3) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "update-alternatives: using /usr/bin/python3.8 to provide /usr/bin/python3 (python3) in auto mode\n",
            "Python 3.8.12\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 2108k  100 2108k    0     0  9994k      0 --:--:-- --:--:-- --:--:-- 9994k\n",
            "Collecting pip\n",
            "  Downloading pip-21.3.1-py3-none-any.whl (1.7 MB)\n",
            "     |████████████████████████████████| 1.7 MB 9.9 MB/s            \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Downloading setuptools-60.5.0-py3-none-any.whl (958 kB)\n",
            "     |████████████████████████████████| 958 kB 87.3 MB/s            \n",
            "\u001b[?25hCollecting wheel\n",
            "  Downloading wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Installing collected packages: wheel, setuptools, pip\n",
            "Successfully installed pip-21.3.1 setuptools-60.5.0 wheel-0.37.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting git+https://github.com/JustAnotherArchivist/snscrape.git\n",
            "  Cloning https://github.com/JustAnotherArchivist/snscrape.git to /tmp/pip-req-build-w4ed789d\n",
            "  Running command git clone --filter=blob:none -q https://github.com/JustAnotherArchivist/snscrape.git /tmp/pip-req-build-w4ed789d\n",
            "  Resolved https://github.com/JustAnotherArchivist/snscrape.git to commit 7f8867825330ee9368ff3284d02033e3e58ab90b\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting requests[socks]\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "     |████████████████████████████████| 63 kB 1.4 MB/s             \n",
            "\u001b[?25hCollecting lxml\n",
            "  Downloading lxml-4.7.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.9 MB)\n",
            "     |████████████████████████████████| 6.9 MB 19.0 MB/s            \n",
            "\u001b[?25hCollecting beautifulsoup4\n",
            "  Downloading beautifulsoup4-4.10.0-py3-none-any.whl (97 kB)\n",
            "     |████████████████████████████████| 97 kB 7.0 MB/s             \n",
            "\u001b[?25hCollecting filelock\n",
            "  Downloading filelock-3.4.2-py3-none-any.whl (9.9 kB)\n",
            "Collecting pytz\n",
            "  Downloading pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
            "     |████████████████████████████████| 503 kB 85.4 MB/s            \n",
            "\u001b[?25hCollecting soupsieve>1.2\n",
            "  Downloading soupsieve-2.3.1-py3-none-any.whl (37 kB)\n",
            "Collecting certifi>=2017.4.17\n",
            "  Downloading certifi-2021.10.8-py2.py3-none-any.whl (149 kB)\n",
            "     |████████████████████████████████| 149 kB 86.8 MB/s            \n",
            "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
            "  Downloading urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
            "     |████████████████████████████████| 138 kB 69.3 MB/s            \n",
            "\u001b[?25hCollecting idna<4,>=2.5\n",
            "  Downloading idna-3.3-py3-none-any.whl (61 kB)\n",
            "     |████████████████████████████████| 61 kB 7.4 MB/s             \n",
            "\u001b[?25hCollecting charset-normalizer~=2.0.0\n",
            "  Downloading charset_normalizer-2.0.10-py3-none-any.whl (39 kB)\n",
            "Collecting PySocks!=1.5.7,>=1.5.6\n",
            "  Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
            "Building wheels for collected packages: snscrape\n",
            "  Building wheel for snscrape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for snscrape: filename=snscrape-0.4.3.20220107.dev8+g7f88678-py3-none-any.whl size=59385 sha256=1c36b73ad48789308a750697694e30e49352f6fcdfc946e1fb5585f4a9ec1d83\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-3y68c5i0/wheels/92/42/87/33fa9b18f7a75d02643a9ca3743339aec9be28c6796267c7d8\n",
            "Successfully built snscrape\n",
            "Installing collected packages: urllib3, idna, charset-normalizer, certifi, soupsieve, requests, PySocks, pytz, lxml, filelock, beautifulsoup4, snscrape\n",
            "Successfully installed PySocks-1.7.1 beautifulsoup4-4.10.0 certifi-2021.10.8 charset-normalizer-2.0.10 filelock-3.4.2 idna-3.3 lxml-4.7.1 pytz-2021.3 requests-2.27.1 snscrape-0.4.3.20220107.dev8+g7f88678 soupsieve-2.3.1 urllib3-1.26.8\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting pandas\n",
            "  Downloading pandas-1.3.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "     |████████████████████████████████| 11.5 MB 8.4 MB/s            \n",
            "\u001b[?25hCollecting numpy>=1.17.3\n",
            "  Downloading numpy-1.22.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "     |████████████████████████████████| 16.8 MB 345 kB/s             \n",
            "\u001b[?25hRequirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (2021.3)\n",
            "Collecting python-dateutil>=2.7.3\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "     |████████████████████████████████| 247 kB 60.1 MB/s            \n",
            "\u001b[?25hCollecting six>=1.5\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, python-dateutil, numpy, pandas\n",
            "Successfully installed numpy-1.22.1 pandas-1.3.5 python-dateutil-2.8.2 six-1.16.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datetime\n",
            "  Downloading DateTime-4.3-py2.py3-none-any.whl (60 kB)\n",
            "     |████████████████████████████████| 60 kB 4.3 MB/s             \n",
            "\u001b[?25hCollecting zope.interface\n",
            "  Downloading zope.interface-5.4.0-cp38-cp38-manylinux2010_x86_64.whl (259 kB)\n",
            "     |████████████████████████████████| 259 kB 18.4 MB/s            \n",
            "\u001b[?25hRequirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (from datetime) (2021.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from zope.interface->datetime) (60.5.0)\n",
            "Installing collected packages: zope.interface, datetime\n",
            "Successfully installed datetime-4.3 zope.interface-5.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.8/dist-packages (2021.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting tweepy\n",
            "  Downloading tweepy-4.4.0-py2.py3-none-any.whl (65 kB)\n",
            "     |████████████████████████████████| 65 kB 2.8 MB/s             \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.11.1 in /usr/local/lib/python3.8/dist-packages (from tweepy) (2.27.1)\n",
            "Collecting requests-oauthlib<2,>=1.0.0\n",
            "  Downloading requests_oauthlib-1.3.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.11.1->tweepy) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.11.1->tweepy) (3.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.11.1->tweepy) (2.0.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.11.1->tweepy) (1.26.8)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.1.1-py2.py3-none-any.whl (146 kB)\n",
            "     |████████████████████████████████| 146 kB 18.1 MB/s            \n",
            "\u001b[?25hInstalling collected packages: oauthlib, requests-oauthlib, tweepy\n",
            "Successfully installed oauthlib-3.1.1 requests-oauthlib-1.3.0 tweepy-4.4.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update \n",
        "!sudo apt-get install python3.8 \n",
        "\n",
        "#change alternatives\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.8 2\n",
        "\n",
        "#check python version\n",
        "!python --version\n",
        "\n",
        "! curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py \n",
        "! python3 get-pip.py --force-reinstall \n",
        "\n",
        "! pip install git+https://github.com/JustAnotherArchivist/snscrape.git\n",
        "\n",
        "! pip install pandas \n",
        "! pip install datetime\n",
        "! pip install pytz\n",
        "! pip install tweepy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prepared_code = \"\"\"\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "from random import random\n",
        "from datetime import date\n",
        "from multiprocessing.dummy import Pool\n",
        "import time\n",
        "\n",
        "# https://github.com/JustAnotherArchivist/snscrape/blob/master/snscrape/modules/twitter.py\n",
        "\n",
        "class Twitter_scraper:\n",
        "  \n",
        "  def __init__(self,\n",
        "               max_results: int,\n",
        "               all_words = [],        # Example: ['what’s', 'happening'] · contains both “what’s” and “happening”\n",
        "               exact_pharase=[],      # Example: ['happy hour'] · contains the exact phrase “happy hour”\n",
        "               any_words = [],        # Example: ['cats', 'dogs'] · contains either “cats” or “dogs” (or both)\n",
        "               none_words = [],       # Example: ['cats', 'dogs'] · does not contain “cats” and does not contain “dogs”\n",
        "               hashtags = [],         # Example: ['#ThrowbackThursday'] or ['ThrowbackThursday'] · contains the hashtag #ThrowbackThursday\n",
        "               mentioned_users = [],  # Example: ['@SFBART', '@Caltrain'] or ['SFBART', 'Caltrain'] · mentions @SFBART or mentions @Caltrain\n",
        "               from_users = [],       # Example: ['@Twitter'] or ['Twitter'] · sent from @Twitter\n",
        "               to_users = [],         # Example: ['@Twitter'] or ['Twitter'] · sent in reply to @Twitter\n",
        "               with_links = True,\n",
        "               with_replies = True,\n",
        "               **kwargs):\n",
        "    \n",
        "    self.number_of_user = 0\n",
        "    self.max_results = max_results\n",
        "    self.all_words = Twitter_scraper.all_of_these_words(all_words)\n",
        "    self.exact_pharase = f'\\\"{exact_pharase}\\\"' if exact_pharase else ''\n",
        "    self.any_words = Twitter_scraper.any_of_these_words(any_words)\n",
        "    self.none_words = Twitter_scraper.none_of_these_words(none_words)\n",
        "    self.these_hashtags = Twitter_scraper.any_of_these_hashtags(hashtags)\n",
        "    self.mentioned_users = Twitter_scraper.mentioning_these_users(mentioned_users)\n",
        "    self.with_links = f'-filter:links' if not with_links else ''\n",
        "    self.with_replies = f'-filter:replies' if not with_replies else ''\n",
        "\n",
        "    self.query_dict = {'all_words':self.all_words, 'exact_pharase':self.exact_pharase,\n",
        "                  'any_words':self.any_words, 'none_words':self.none_words, 'these_hashtags':self.these_hashtags,\n",
        "                  'mentioned_users':self.mentioned_users, 'with_links': self.with_links, 'with_replies':self.with_replies}\n",
        "\n",
        "    self.query_dict['from'] = Twitter_scraper.f_or_t_users(from_users, 'from')\n",
        "    self.query_dict['to'] = Twitter_scraper.f_or_t_users(to_users, 'to')\n",
        "\n",
        "    for key, value in kwargs.items():\n",
        "        self.query_dict[key] = (f'({key}:{value})')\n",
        "\n",
        "\n",
        "  @staticmethod\n",
        "  def f_or_t_users(users, key):\n",
        "    if not users:\n",
        "      return ''\n",
        "    tmp_list = [f'{key}:{user}' for user in users]\n",
        "    return('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def all_of_these_words(all_words):\n",
        "    if not all_words:\n",
        "      return ''\n",
        "    return ' '.join(all_words)\n",
        "\n",
        "  @staticmethod\n",
        "  def any_of_these_words(any_words):\n",
        "    if not any_words:\n",
        "      return ''\n",
        "    return ('(' + ' OR '.join(any_words) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def none_of_these_words(none_words):\n",
        "    if not none_words:\n",
        "      return ''    \n",
        "    return ('-' + ' -'.join(none_words))\n",
        "\n",
        "  @staticmethod\n",
        "  def any_of_these_hashtags(hashtags):\n",
        "    if not hashtags:\n",
        "      return ''    \n",
        "    tmp_list = ['#'+ h.replace('#','') for h in hashtags]\n",
        "    return ('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  @staticmethod\n",
        "  def mentioning_these_users(users):\n",
        "    if not users:\n",
        "      return ''    \n",
        "    tmp_list = ['@'+ h.replace('@','') for h in users]\n",
        "    return ('(' + ' OR '.join(tmp_list) + ')')\n",
        "\n",
        "  def create_query(self, query_dict):\n",
        "    tmp_string = ''\n",
        "    res = dict([(key, val) for key, val in \n",
        "           query_dict.items() if val])\n",
        "    del query_dict\n",
        "    query = ' '.join(res.values())\n",
        "    del res\n",
        "\n",
        "    return query\n",
        "\n",
        "\n",
        "  def crawler(self, query, error_counter=0):\n",
        "    # Creating list to append tweet data\n",
        "    tweets_list = []\n",
        "    try:\n",
        "      # Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "      scraper = sntwitter.TwitterSearchScraper(query)\n",
        "      i = 0\n",
        "      for tweet in scraper.get_items(): #declare a username\n",
        "          if i >= self.max_results: #check number and date\n",
        "            break\n",
        "\n",
        "          tweets_list.append([tweet.date, tweet.id, tweet.content, tweet.replyCount, tweet.retweetCount,\n",
        "                tweet.likeCount, tweet.user.username, tweet.lang, tweet.media, tweet.hashtags]) #declare the attributes to be returned\n",
        "          i += 1\n",
        "      \n",
        "    except Exception as e:\n",
        "      if 'Unable to find guest token' in str(e):\n",
        "          error_counter += 1\n",
        "          if error_counter > 3:\n",
        "            error_counter = 0\n",
        "            print(\"Sleep Time!\")\n",
        "            time.sleep(30.3 *60)\n",
        "            print(\"Morning!\")\n",
        "\n",
        "          return self.crawler(query, error_counter)\n",
        "\n",
        "      print(f\"query: {query} , {e}\")\n",
        "\n",
        "    # Creating a dataframe from the tweets list above \n",
        "    tweets_df = pd.DataFrame(tweets_list, columns=['Datetime', 'Tweet Id', 'Text', 'Reply Count',\n",
        "                                                    'Retweet Count', 'Like Count', 'Username', 'Lang', 'Media', 'Hashtags'])\n",
        "    return tweets_df\n",
        "\n",
        "\n",
        "  def basic_mode(self):\n",
        "    query = self.create_query(self.query_dict)\n",
        "    return self.crawler(query)\n",
        "\n",
        "  def user_crawler(self, user):\n",
        "    tmp_dict = self.query_dict.copy()\n",
        "    tmp_dict['from'] = (f'(from:{user})')\n",
        "    query = self.create_query(tmp_dict)\n",
        "    del tmp_dict\n",
        "    return self.crawler(query)\n",
        "\n",
        "\n",
        "  def user_mode(self, user_list):   \n",
        "    user_crawler = self.user_crawler \n",
        "    pool = Pool(22)\n",
        "    df_list = pool.map(user_crawler, user_list)\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    result_df = pd.concat(df_list, ignore_index=True)\n",
        "    return result_df\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from_users = [] #['CNN', 'FoxNews', 'ABC', 'BBCWorld', 'TIME', 'CBSNews', 'NBCNews', 'MSNBC','nytimes','washingtonpost']\n",
        "scraper = Twitter_scraper(max_results=10**4, all_words=['migraine'],any_words=['headache'],until=\"2020-01-01\", since=\"2019-01-01\", lang=\"en\", from_users=from_users, with_replies=False)\n",
        "result = scraper.basic_mode()\n",
        "from collections import Counter\n",
        "text = result['Text'].str.lower()\n",
        "migraine=pd.DataFrame(Counter(\" \".join(text).split()).most_common(200))\n",
        "migraine.to_csv('migraine.csv',index=False)\n",
        "print(migraine)\n",
        "result.to_csv('test.csv', index=False)\n",
        "print(result.shape)\n",
        "\n",
        "# Example:\n",
        "\n",
        "#all_words = ['ایران', 'است'] # contains both “ایران” and “است”\n",
        "\n",
        "#lang=\"fa\"\n",
        "\n",
        "#scraper = Twitter_scraper(max_results=10**11, until=\"2020-01-01\", since=\"2019-01-01\", lang=lang, with_replies=False)\n",
        "#result = scraper.basic_mode()\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(\"python.py\", \"w\") as f:\n",
        "  f.write(prepared_code)\n",
        "\n"
      ],
      "metadata": {
        "id": "2LDAEJjaxNqN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python python.py"
      ],
      "metadata": {
        "id": "glB1gSrrxNcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62e1f2f1-ea9f-40dd-9330-6fde771d36e0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             0     1\n",
            "0            a  8237\n",
            "1          and  6684\n",
            "2            i  5963\n",
            "3          the  5810\n",
            "4           to  5440\n",
            "..         ...   ...\n",
            "95      listen   334\n",
            "96   treatment   333\n",
            "97       still   328\n",
            "98  #migraines   327\n",
            "99        than   327\n",
            "\n",
            "[100 rows x 2 columns]\n",
            "(10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "NUwT-wocs8pL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = pd.read_csv('test.csv')"
      ],
      "metadata": {
        "id": "uRnPZM5KtMJW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = result['Text'].str.lower()"
      ],
      "metadata": {
        "id": "ZcjLKD3btNqx"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "migraine = pd.DataFrame(Counter(\" \".join(text).split()).most_common(200))\n",
        "migraine.to_csv('migraine.csv',index=False)\n",
        "print(migraine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSAwbx8QtYze",
        "outputId": "40eab688-523b-454e-9879-31bd1a0de09a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           0     1\n",
            "0          a  8237\n",
            "1        and  6684\n",
            "2          i  5963\n",
            "3        the  5810\n",
            "4         to  5440\n",
            "..       ...   ...\n",
            "195  cluster   162\n",
            "196     week   162\n",
            "197      too   162\n",
            "198    learn   161\n",
            "199     turn   159\n",
            "\n",
            "[200 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.corpus\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEouFSGLuPHZ",
        "outputId": "c27e5977-d11f-4544-a33e-584f7e419e99"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "migraine['clean'] = migraine[0].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
      ],
      "metadata": {
        "id": "k0jaNfinuHma"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYMZGpMQAeaq",
        "outputId": "946a71a2-9cde-4822-e80c-c21b9a4b495e"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, row in migraine.iterrows():\n",
        "  if row[0] in stop_words: \n",
        "    migraine.drop(i, inplace=True)  "
      ],
      "metadata": {
        "id": "KZql_A5iuV7E"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "migraine.to_csv('migraine_clean.csv',index=False)"
      ],
      "metadata": {
        "id": "ZYuQLAY1_rVk"
      },
      "execution_count": 82,
      "outputs": []
    }
  ]
}